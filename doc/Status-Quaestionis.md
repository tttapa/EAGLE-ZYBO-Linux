# Status Quaestionis

## ZYBO Linux

For compiling C/C++ applications for the ZYBO core running Linux, a GNU
Toolchain is used. This toolchain is available as a Docker image for maximum
portability, because installing GCC etc. on Windows is not trivial.

The Docker image also contains a native x86-64 toolchain for testing and
debugging. This allows you to run the applications not only on the ZYBO, but on
any normal computer as well.

All build tasks are available as tasks in Visual Studio Code, this includes
building the Docker images, toolchains, libraries, ZYBO applications, testing,
debugging.

### Crypto

âœ“ I provided the basic structure for the Crypto code. It just writes a
single word to R0, and then reads it back from R1.

âœ“ I included the ZBar library for decoding the QR code. I also tried the OpenCV
QR decoder, but ZBar seems to work better. I added a test as an example to run
it asynchronously, that is, vision keeps on refreshing while the QR decoder is
running. It remains to be seen if this is a good idea, because we only have a 
single thread anyway.

âœ“ I also added a base64-decoder library for extracting the binary data from the 
QR code.

ğŸ—™ The next step is to write functions that are able to decode strings of data, 
as well as more elaborate test cases.

The public API of the crypto code goes in `src/crypto/include/crypto.hpp`, and
the implementations in `src/crypto/src/crypto.cpp`.  
Test cases go in `src/crypto/test/test-crypto.cpp`.

ğŸ—™ The reading and writing of the FPGA registers uses the provided code in
`src/comm/src/wrapper.c`, from [Xilinx Application note 1078](https://www.xilinx.com/support/documentation/application_notes/xapp1078-amp-linux-bare-metal.pdf).  
This is not ideal, because it has a lot of overhead when reading large amounts
of data. The code is simple enough, and adding a function for doing longer
transactions to the FPGA is a good idea, instead of calling the `rmem()` and
`wmem()` functions for each word separately.

âœ“ Tuur and Cyriel are working on the code that reads and decodes the QR code, 
and sends the data to crypto.

### Logging

The logging on the Linux side is pretty basic: it just reads the size of the log
entry from shared memory (`wrapper.h`), and then reads this many words and sends
them as a multicast UDP packet.

âœ“ The multicast sender has been implemented and tested.

ğŸ—™ The part that reads from shared memory is trivial, but not finished yet,
we'll have to finish the logger in Baremetal first.  
It's important that we use different locks for logging and vision.

ğŸ—™ If we can't get multicast routing to work for the base station, we might have
to send the same message as unicast as well.

### Vision

âœ“ The main part of the vision algorithm is GridFinder: as an input, it uses the
mask, where red lines are white, and everything else is black. As an output, it
returns the vertices of the square of the grid in the center of the frame, and
the lines (and angles) of that square.  
This part is finished, and has been tested using videos.

âœ“ I've completely re-written the red pixel detection for generating the masks.  
It now uses ARM NEON SIMD vector instructions, so it can mask 16 pixels in 
parallel. There's also a function that's a bit more accurate, but a bit slower.
If dark, washed-out red is detected as a line, and if this is a problem, we 
could use that code instead.

âœ“ Reading the image from the HDMI input seems to be working. There is a purple
line at the edge of the frame though, that I can't seem to get rid of.

âœ“ Getting the absolute position of the drone seems to be working. The entire
vision program runs at 60 fps on the ZYBO, but every two frames are the same,
because the camera only updates at 30 fps.

ğŸ—™ Finally, this location has to be sent to the BareMetal core. This is trivial.

## ZYBO Baremetal

### BaremetalImproved framework

âœ“ src-vivado is rewritten  
ğŸ—™ corrected functionality in src-vivado (e.g. IMU calibration, IMU reading,
AHRS orientation, clamp RC inputs) needs to be verified  
â“ src is nearly finished  
ğŸ—™ the entire framework needs to be debugged  

### Controllers

âœ“ Controllers simulated  
âœ“ Code generator working  
â“ Attitude is tuned, but needs fine tuning for new battery  
â“ Altitude is tuned, but needs fine tuning for new battery  
â“ Navigation controller needs tuning  
ğŸ—™ Observer for navigation that only uses the accelerometer measurements 
(for taking off/landing when there's no vision data)

### Autopilot

â“ Autonomous navigation has been programmed, it will be tested as soon as we 
finish tuning the navigation controller  
â“ Automatic take-off and landing has been implemented, but not tested yet

### Logger

âœ“ Code generator working  
â“ Update code generator with new BaremetalImproved  

## Videos for technical milestones T4

### 1. Flying

âœ“ Stable flight demonstrated for 10s  
âœ“ Pilot in control of flight moving left, right, forward, backward, up and down

### 2. ESCs

âœ“ Demonstrate at least one working and integrated ESC. The pilot is able to 
control motor throttle via the remote controller.  
ğŸ—™ Flying with own ESCs

### 3. Altitude controller

âœ“ Flying for 10s while only manually controlling X-Y  
âœ“ Dito, but with very stable achieved (<30cm variation) at most 20 seconds after 
enabling height control and maintained for at least 20 seconds

### 4. Loitering

ğŸ—™ Staying autonomously (more or less) within a 3x3 square grid without manual
x-y control for 10s  
ğŸ—™ Long term (>10s) stable location  within a single square, proven by
automatic landing within a square (on the coil)

### 5. Wireless video link

â“ Wireless video link established and link performance analysis done  
ğŸ—™ Wireless video link parameters tuned autonomously

### 6. Logging

â“ Real-time logging info during flight  
ğŸ—™ Very informative and visually attractive logging, graphically plotting info
at real time during flight

### 7. Navigation (walking)

ğŸ—™ Decode current X-Y coordinates in the grid at any square (also those without 
QR code), and decode location targets from QR codes while walking over the grid 
with drone, being correct over at least 10 squares and 2 QR codes. Results 
should show up on GUI  
ğŸ—™ Do the same with a hardware accelerated crypto implementation

### 8. Navigation (flying)

ğŸ—™ Real-time localization during actual flight  
ğŸ—™ Flying/navigating fully autonomously following the QR trail of the mission

### 9. WPT

âœ“ Capable of fully lighting up the LED wall after manually putting the drone on 
the coil, with on/off activation with the remote control.  
âœ“ Lighting up the LED wall even when the two coils are aligned only 50%, e.g.
after automated landing.