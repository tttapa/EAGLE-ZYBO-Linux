<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.16"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>EAGLE ZYBO Linux: Presentation</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
  $(document).ready(initResizable);
/* @license-end */</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/SVG"],
});
</script><script type="text/javascript" async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="style.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">EAGLE ZYBO Linux
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.16 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('md_Presentation.html','');});
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Presentation </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>Things to brag about</h2>
<h3>Baremetal</h3>
<p>We have completely re-written the original C code for the drone controller. We're using a <b>modern GCC toolchain</b>, instead of the outdated version that came with the Xilinx SDK. Using modern <b>C++</b> allowed us to write much <b>cleaner, easier to read code, with fewer bugs</b>. The most important bug fix was the <b>sensor system</b>. Large mathematical errors were not seen by the authors, and because of this the orientation measurement finished. <b>Many small bugs from the original code are now fixed</b> and <b>extra structure and functionality</b> has been added. The we fixed and some mathematical errors were somehow not seen by the original authors.were some <b>bugs and mathematical errors</b> in the sensor system that we fixed.</p>
<p>We used a complete cross-compilation toolchain with a cross-platform build system, so we can develop and run unit tests on our computer.</p>
<p>We have done <b>performance checks</b> on the new and old programs, and we can confirm that after switching to C++, our program still runs at exactly the same speed, and the increase in readability and maintainability is huge.</p>
<p>We're using a different <b>sensor fusion algorithm</b> for the IMU, that performs much better than the given code.</p>
<p>We are now able to run the IMU input attitude controller at almost <b>1 kHz</b> instead of the original 238 Hz, so we get less drift, and a more stable controller.</p>
<p>The <b>controller code is automatically generated</b> using a MATLAB script that allows us to <b>easily tune any parameter</b>. However, uploading the new code to the drone was a tedious task (creating a boot image in Vivado on an ESAT PC, turn off the drone, take out the SD card, run to the computer with it, copy the boot image to the SD card, safely remove, run back to the drone, plug it in, wait for it to boot, etc.)</p>
<p>Our solution to this problem is two-fold: 1) use a faster way to upload new code 2) tune without having to re-upload code.</p>
<p>For <b>faster uploads</b>, we use <b>SSH</b>. This means that we don't need physical access to the drone, and we don't have to unplug the battery. We just use <code>SCP</code> to upload the boot image, and then reboot the ZYBO over <code>SSH</code>. Because we don't unplug the battery, we don't have to wait over five minutes for the WiFi router to boot.</p>
<p>We also added <b>multiple slots of controllers</b> to our code generator. So we can try multiple controllers by changing the configuration using the tune knob on the remote controller. This means that we don't have to re-upload for every tuning setting we want to try.</p>
<h3>Vision</h3>
<p>The first approach used the Hough transform in OpenCV Python. However, this was too slow, so the algorithm skipped some squares when moving or tilting too fast. A second approach just counted the number of red pixels on each row and each column. This was a bit faster, but it only worked if the drone didn't rotate about the z-axis. This algorithm still occasionally skipped a square, making it not suitable for a loitering controller. We tried speeding things up even more by down-sampling the images, but no avail.</p>
<p>Another problem for vision was the given Python framework. It had some serious issues, such as deadlock between the vision and logger thread, it didn't release the video input when something went wrong, and it degraded the performance even further.</p>
<p>After countless nights of trying to get the drone to loiter, we decided to start from scratch. We threw out the Python framework in favor of a much more light-weight C++ application, and we created a very ingenious grid finder algorithm in C++ as well.</p>
<p>For development of the vision algorithm on the PC, we used <a class="el" href="namespacepybind11.html">pybind11</a>. This allowed us to use our C++ algorithm in Jupyter notebooks in Python, having access to all the benefits of a scripting language, and modules like matplotlib and OpenCV for easy debugging, while still keeping super fast C++ performance.</p>
<p>Once we had the algorithm figured out, and we could show it working on the given videos, we started setting up a cross-compilation toolchain for the ZYBO. <br  />
We decided to use a Docker build container, so it could be used by all team members, regardless of the operating system they were using. The toolchain consists of a custom GCC build for the ZYBO, with cross-compiled versions of the OpenCV and ZBar libraries. Native x86 versions of all tools are available as well, so we can do the initial development on the computer, without requiring access to the ZYBO. <br  />
All unit tests (that don't require platform-specific features like the FPGA) are run natively in the Docker container as well. <br  />
The entire toolchain is tightly integrated with Visual Studio Code: there are shortcuts for quickly building the entire project for ARM or for x86, for running the unit test on the PC or on the ZYBO, for getting a shell on the ZYBO, for starting a debug session on the ZYBO, etc. <br  />
CMake ensures that all libraries and binaries are up to date, and they are automatically synced with the drone over SSH. <br  />
For unit testing, the Google Test framework is used. Automatic documentation generation is handled by Doxygen. A Python Jupyter environment and Python bindings for the C++ libraries we wrote are available as well, for easy visualization of the vision algorithm. All of this is contained within the Docker container.</p>
<p>Once we got our grid finder algorithm working on the ZYBO, it became clear that we had to speed up the masking as well: the new algorithm to find the lines ran at 800 fps, but the masking (detecting the red pixels in the image) only ran at 50 fps. We got rid of the OpenCV functions for masking, and wrote our own version in C++. We're using the ARM <a class="el" href="namespaceNEON.html">NEON</a> SIMD vector unit of the ZYBO, using intrinsics, meaning that we can mask 16 pixels in parallel. At the full resolution of 640×480, the masking now runs at almost 1000 fps.</p>
<p>All of this means that we're able to update the position measurement of the drone at 60 fps, which is the refresh rate of the HDMI input. This enables us to design better controllers, and ensures that we don't skip any squares, even when moving very quickly.</p>
<p>The grid finder algorithm is very robust. It can handle cases where certain parts are overexposed by direct sunlight, gaps in the lines, curved lines due to lens distortion, and it is mathematically correct even when turning the drone 360° around the z-axis.</p>
<p><a href="drone-images-sunlight+mask.out.mp4">Video sunlight</a> <br  />
<a href="DroneCam-Spinning.out.mp4">Video 360°</a></p>
<p>The final Linux application consists of three main parts: the vision algorithm for the localization, the <a class="el" href="structQR.html">QR</a> decoding using ZBar and the algorithm developed by our crypto team, that runs on the FPGA, and the data logger for ANC.</p>
<p>Decoding the <a class="el" href="structQR.html">QR</a> code happens asynchronously, so it doesn't interrupt the vision algorithm (it does slow it down slightly). The logger doesn't really require any processing, it just reads whatever it finds in the shared memory from ANC, and sends it over the network as UDP Multicast. It runs in a separate thread, and runs periodically. </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.16 </li>
  </ul>
</div>
</body>
</html>
